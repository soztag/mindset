--- 
title: "Mindset"
author: 
  - name: "Marco Blank"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Maximilian Held"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Manuel Nicklich"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Sabine Pfeiffer"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Stefan Sauer"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Amelie Tihlarik"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: SOZTAG-EXPL2019.bib
editor_options: 
  chunk_output_type: console
---

<div class="jumbotron" style="color:white; background: linear-gradient( rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5) ), no-repeat center center fixed; -webkit-background-size: cover; -moz-background-size: cover; -o-background-size: cover; background-size: cover;">
  <h2>Mindset</h1>
  <p>Text Mining Trends</p>
  <p>
    <span class="label label-info">
      #mindset
    </span>
  </p>
</div>


```{r, child="README.md"}
```

---

```{r setup, cache = FALSE, include = FALSE}
source("setup.R")
```

<div class="alert alert-warning">
**Early Draft; Don't Cite or Circulate**
</div>

## Buzz


### Google Books

```{r books}
library(ngramr)
ngram2 <- function(phrases, corpus) {
  ng <- ngram(
    phrases = phrases,
    corpus = corpus,
    year_start = 1945,
    year_end = 2019,
    case_ins = TRUE,
    smoothing = 0
  )
  ng$Phrase <- as.character(tolower(ng$Phrase))
  ng$Corpus <- as.character(tolower(ng$Corpus))
  ng
}
corpora <- c(german = "ger_2012", english = "eng_2012")
phrases <- c("mindset")
combs <- expand.grid(phrases = phrases, corpus = corpora, stringsAsFactors = FALSE)
ng <- purrr::pmap_dfr(
  .l = combs,
  .f = ngram2
)
library(dplyr)
ng <- ng %>% 
  group_by(Year, Phrase, Corpus) %>% 
  summarise(Frequency = sum(Frequency))
library(ggplot2)
ggplot(
  data = ng,
  mapping = aes(x = Year, y = Frequency, colour = Phrase, linetype = Corpus)
) +
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, na.rm = TRUE, span = 0.2) + 
  xlab(lab = "Jahr") + ylab(label = "Häufigkeit") +
  scale_linetype_discrete(labels = c("Englisch", "Deutsch")) + 
  labs(linetype = "Korpus", subtitle = "Foo") + labs(colour = "Term") +
  ggtitle(label = "'Mindset' in Büchern", subtitle = "Unigrams aus Google Books 2012 Korpora (1945-2010, LOESS)")
```

Der Begriff des "Mindsets" wird in in den letzten Jahrzehnten zunehmenden in gedruckten Werken verwandt, beginnend in englischen Veröffentlichungen in den 1980er Jahren, in deutschsprachigen Büchern dann ab den 2000er Jahren.
Abgebildet sind hier die relativen Häufigkeiten der Unigramme in den jeweiligen Korpora.
Spezifischere n-Grams wie etwa "growth mindset" (nach @Dweck2017) kommen leider in den zur Verfügung stehenden Korpora bis 2010 nicht vor.


## Google Trends 

```{r, fig.cap="Google Trends von FGM Begriffen (LOESSS)"}
geos <- c(Welt = NA, Deutschland = "DE", USA = "US")
gtrends2 <- function(geo, subset = "interest_over_time") {
  trends <- gtrendsR::gtrends(
    keyword = c("mindset", "growth mindset", "fixed mindset", "agile mindset", "digital mindset"),
    time = "all",
    geo = geo,
    low_search_volume = FALSE
  )
  trends <- trends[[subset]]
  trends <- tibble::as_tibble(trends)
  trends$hits <- as.integer(trends$hits)
  trends$hits[is.na(trends$hits)] <- 0
  trends
}
interest_over_time <- purrr::map_dfr(
  .x = geos,
  .f = gtrends2,
  subset = "interest_over_time",
  .id = "browser_geo"
)
ggplot(data = interest_over_time, mapping = aes(x = date, y = hits, color = keyword)) + 
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, na.rm = TRUE, span = 0.2) + 
  facet_wrap(~browser_geo, ncol = 1) +
  scale_y_sqrt(limits = c(0, 100)) +
  ylab("Relative Häufigkei, Indiziert auf 100") +
  labs(colour = "Suchanfrage") +
  ggtitle(label = "Suchanfragen auf Google (Quadratisch)", subtitle = "Ort nach Browser Geolocation")
```

Die Abbildung gibt die weltweiten Google-Anfragen zu den aufgelisteten Konzepten wieder, indexiert auf einen (rohen) Höchstwert von 100.
Die Rohdaten sind allerdings recht verrausscht, daher wird hier eine erheblich geglättete Form präsentiert (LOESS), in der der Höchstwert nicht mehr enthalten ist.

Wie man in der o.s. Abbildung sieht, wird Mindset im Kontext von Personalmanagement sowohl in den USA, als auch Deutschland und im Rest der Welt etwa seit 2012 verstärkt als Suchbegriff verwendet.
Die Unterkonzepte wie "agile Mindset" und "digital Mindset" scheinen vor allem in Deutschland gesucht zu werden, was aber möglicherweise ein Artefakt der Indizierung ist: 
Da "Mindset" insgesamt in Deutschland sehr viel weniger verwendet wird, erscheint das Wachstum der Unterkonzepte relativ stärker.


```{r fig.cap="Google Trends von FGM Begriffen im Vergleich (LOESSS)", warning = FALSE}
other_trends <- gtrendsR::gtrends(
  keyword = c("scrum master", "data scientist", "social media manager", "growth hacker"),
  time = "all",
  low_search_volume = TRUE
)

# weirdly, above sometimes includes characters ">1" as hits, so this needs to be fixed
other_trends$interest_over_time$hits <- as.integer(other_trends$interest_over_time$hits)
other_trends$interest_over_time$hits[is.na(other_trends$interest_over_time$hits)] <- 0

df <- dplyr::bind_rows(
  list(
    fgm = fgm_gtrends$interest_over_time, 
    others = other_trends$interest_over_time
  ), 
  .id = "group"
) %>% 
  as_tibble()


# now because these two datasets are separately indexed to 100, we need to download the two highest together, to find the right multiplier
highest <- group_by(df, group) %>% 
  filter(hits == max(hits)) %>% 
  select(keyword) %>% 
  deframe()
multiplier <- gtrendsR::gtrends(
  keyword = highest,
  time = "all",
  low_search_volume = TRUE
) %>% 
  extract2("interest_over_time") %>% 
  group_by(keyword) %>% 
  filter(hits == max(hits)) %>% 
  select(hits) %>% 
  deframe() %>% 
  as.integer() %>% 
  min() %>% 
  divide_by(100)

df <- mutate(
  .data = df,
  hits = if_else(
    condition = group == "fgm",
    true = hits * multiplier,
    false = hits
  )
)

g <- ggplot(
  data = df,
  mapping = aes(
    x = date,
    y = hits,
    color = keyword,
    linetype = group
  )
) +
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, na.rm = TRUE, span = 0.3) +
  ylim(0, 100) +  # because google trends are indexed to 100
  scale_x_date(date_breaks = "1 year") + 
  guides(color = guide_legend(ncol = 2)) + 
  theme(
    legend.position = "bottom", 
    axis.text.x = element_text(angle = 90, hjust = 1)
  )
g
```



```{r import_via_praw, eval=FALSE, include=FALSE}
library(RedditExtractoR)
urls <- RedditExtractoR::reddit_urls(search_terms = "'bullshit jobs'", cn_threshold = 1) %>% 
  as_tibble() %>% 
  select(title, subreddit, URL) %>% 
  mutate(submission_id = map_chr(.x = stringr::str_split(string = urls$URL, pattern = "/"), 7))

library(reticulate)
os <- import(module = "os")  # just to access the os via python and figure out current wd
checkmate::assert_true(x = getwd() == os$getcwd()) 
praw <- import(module = "praw")
# this calls credentials stored in praw.ini, but gitignored here
# as per https://praw.readthedocs.io/en/latest/getting_started/configuration/environment_variables.html
py_objs <- py_run_file(file = "praw.py", convert = TRUE)

# loop over all urls
map2_dfr(
  .x = urls$submission_id,
  .y = urls$title, 
  .f = function(submission_id, submission_title) {
    py_objs$getAllForOneURL(submission_id) %>% 
      discard(.p = function(x) {
        # these are just "more comments" buttons
        inherits(x = x, what = "praw.models.reddit.more.MoreComments")
      }) %>% 
      map_dfr(.id = "index", .f = function(x, y) {
        tibble(
          submission_id = submission_id,
          id = x$id,
          body = x$body,
          score = x$score,
          # sometimes authors are NULL for some reason
          author = ifelse(test = is.null(x$author$name), yes = NA, no = x$author$name),
          submission_title = submission_title
        )
      })
  }
) %>% 
  readr::write_rds(path = "r_coms.rds")
```


```{r read}
r_coms <- readr::read_rds(path = "r_coms.rds")
```

<!-- Vor allem aus forschungspraktischen Gründen nutzen wir hierzu Einträge auf dem Social News Aggregator [reddit.com](http:://www.reddit.com): -->
<!-- Die Plattform bietet für Forschende großzügige Nutzungsbedingungen, und ist durch eine ausgereifte API mit entsprechendem Python Package gut erschlossen [@BoePythonRedditAPI2016]. -->
<!-- So war es möglich in recht knapper Zeit **` r nrow(r_coms)`** Volltext-Kommentare von **` r length(unique(r_coms$author))`** unterschiedlichen Nutzernamen zu **` r length(unique(r_coms$submission_id))`** Einreichungen in einer Vollerhebung zu sammeln. -->


### Analyse

<div class="alert alert-warning">
**Entwurf**
</div>

Zunächst wurden Kommentare in Zeilen, und dann mittels des [tidytext](https://www.tidytextmining.com) package in einzelne Wörter tokenisiert [@SilgeTextMining].
Zudem wurden Synonyme des oft diskutierten UBIs ersetzt.^[Eine umfassendere, programmatische Ersetzung von weiteren Synonymen wäre sinnvoll, ist hier jedoch nicht erfolgt.]
Nach der Entfernung von Stoppwörtern ("and", "it", etc.) wurden die verbleibenden Wörter mittels des `TreeTagger` Programms wie empfohlen probabilistisch lemmaisiert [@SchmidTreetaggerlanguageindependent1995].


```{r clean_words}
r_coms_by_line <- r_coms %>% 
  tidyr::separate_rows(
    body,
    sep = "\n\n"
  ) %>% 
  mutate(body = stringi::stri_replace_all(
    # kind of hacky, but these terms just screw up the results otherwise
    str = body, 
    fixed = c("universal income", "basic income", "universal basic income"), 
    replacement = "ubi", 
    merge = FALSE, 
    vectorize_all = FALSE)
  ) %>% 
  group_by(id) %>% 
  mutate(line = 1:n()) %>% 
  ungroup()
td_coms <- r_coms_by_line %>% 
  unnest_tokens(output = word, input = body, format = "text") %>% 
  anti_join(stop_words) %>% 
  tidyr::separate_rows(  # separate out sepcial charcs
    word,
    sep = "_"
  )
td_coms <- td_coms %>% 
  dplyr::filter(
    is.na(suppressWarnings(as.numeric(word))),
    str_length(word) > 1
  ) %>% 
  mutate(
    word = stringr::str_extract(word, "[a-z']+")
  )

# lemmatization
if (FALSE) {
  # only run this locally, where treetagger is available
  # TODO would be better to do this with proper dep management apt get and homebrew
  withr::with_dir(
    new = "treetagger/",
    code = processx::run(command = "sh", args = "install-tagger.sh"))
  textstem::make_lemma_dictionary(
  td_coms$word,
  engine = "treetagger",
  lang = "en"
  ) %>% 
    readr::write_rds(path = "dict.rds")
}
dict <- readr::read_rds(path = "dict.rds")

td_coms <- td_coms %>% 
  mutate(
    word = textstem::lemmatize_strings(x = word, dictionary = dict),
    submission_title = stringr::str_trunc(
      string = submission_title,
      width = 15,
      side = "right",
      ellipsis = "..."
    )
  )

# TODO replacing with more common synonyms via WordNet might be even better, http://www.bernhardlearns.com/2017/04/cleaning-words-with-r-stemming.html
```

<!-- Es verbleiben über **` r nrow(td_coms)`** Beobachtungen von **` r length(unique(td_coms$word))`** unikalen Wörter, darunter **` r count(x = td_coms, word, sort = TRUE) %>% dplyr::filter(n > 10) %>% nrow()`** mit mehr als 10 Nennungen. -->

```{r, fig.cap="Unigram der über 300 Mal genannten Wörter"}
count(x = td_coms, word, sort = TRUE) %>% 
  dplyr::filter(n > 300) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) + 
  coord_flip()
```


```{r, fig.cap="Vergleich der Unigrams in Prozenten der drei größten Threads", fig.width=9, fig.height=9, warning=FALSE}
td_coms %>% 
  group_by(submission_title) %>% 
  dplyr::filter(n() > 1900) %>% 
  count(submission_title, word) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(submission_title, proportion) %>% 
  gather(submission_title, proportion, 2:6, -"Forget fears...") %>% 
  ggplot(
    mapping = aes(
      x = proportion, 
      y = `Forget fears...`, 
      color = abs(`Forget fears...` - proportion)
    )
  ) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~submission_title, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Forget fears of automation ... (wired.com)", x = NULL)
```

<!-- TODO -->
<!-- Der Vergleich der ausgewählten Threads könnte mit Hilfe einer üblichen $tf-idf$ Transformation noch präziser untersucht werden. -->

Um die oben stehenden Unigrams zu erweitern, bietet sich auch ein Blick auf ein Netzwerk von Bigrams an.

```{r bigram-network, fig.cap="Common bigrams in the reddit comments"}
coms_bigram_graph <- r_coms_by_line %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  # put each in a column, so we can get rid of stop words
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  dplyr::filter(
    !word1 %in% c(stop_words$word, "http", "www", "https"),
    !word2 %in% c(stop_words$word, "http", "www", "https"),
  ) %>%
  # duplication from above
  tidyr::separate_rows(  # separate out sepcial charcs
    word1,
    sep = "_"
  ) %>% 
  tidyr::separate_rows(  # separate out sepcial charcs
    word2,
    sep = "_"
  ) %>% 
  dplyr::filter(
    is.na(suppressWarnings(as.numeric(word1))),
    str_length(word1) > 1,
    is.na(suppressWarnings(as.numeric(word2))),
    str_length(word2) > 1,
  ) %>% 
  mutate(
    word1 = stringr::str_extract(word1, "[a-z']+"),
    word2 = stringr::str_extract(word2, "[a-z']+")
  ) %>%  
  mutate(
    word1 = textstem::lemmatize_strings(x = word1, dictionary = dict),
    word2 = textstem::lemmatize_strings(x = word2, dictionary = dict),
    submission_title = stringr::str_trunc(
      string = submission_title,
      width = 15,
      side = "right",
      ellipsis = "..."
    )
  ) %>% 
  count(word1, word2, sort = TRUE) %>% 
  dplyr::filter(n > 20) %>% 
  igraph::graph_from_data_frame()

library(ggraph)
set.seed(2018)
ggraph(coms_bigram_graph, layout = "fr") +
  geom_edge_link(
    show.legend = FALSE
  ) +
  geom_node_text(mapping = aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()
```

Oben stehend sind häufig vorkommenden (> 20) Bigramme in den reddit Kommentaren in einem Netzwerk abgebildet.
Stoppwörter und Lemmaisierung sind wie oben beschrieben vorgenommen.

Einige der üblichen Ansätze für Textanalyse erscheinen für den vorliegenden Korpus wenig geeignet.
Eine Sentiment-Analyse, zumindest eine auf Unigrams basierende, ist für die vorliegenden Daten mit gängigen Lexika wenig aussagekräftig.
Der spezifische Kontext (hier: bullshit jobs) von Wörtern kann mit diesen Methoden nicht abgebildet werden.

Schließlich extrahieren wir mittels Latent Dirichlet Allocation (LDA) Muster von ähnlichen Thmene zwischen den unterschiedlichen Kommentaren.
Bei einer LDA werden die einzelnen Dokumente (hier: Kommentare) als Mischungen von Themen beschrieben und die einzelnen Themen wiederrum als (möglicherweise überlappende) Mischungen von Wörtern.
Eine LDA daher zerlegt die Rohdaten in zwei Matrizen: Eine Matrix $Wörter x Themen$ mit den Gewichten der Wörter pro Thema, und eine $Dokumente x Themen$ Matrix mit den Gewichten der Themen pro Dokument.
Somit handelt es sich bei der LDA um eine Dimensionsreduktion, also um einen Ansatz des *unsupervised learning*. 

```{r}
coms_lda_td <- td_coms %>% 
  count(submission_id, word) %>% 
  cast_dtm(document = submission_id, term = word, value = n) %>% 
  topicmodels::LDA(control = list(seed = 1234), k = 2) %>% 
  tidy(matrix = "beta")
```

```{r top_ten, fig.cap="20 Most Probable Words for two Extracted Topics."}
coms_lda_td %>% 
  group_by(topic) %>% 
  top_n(20, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta) %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(mapping = aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + 
  coord_flip()
```

```{r greatest_diff, fig.cap="Highest 20 Log Ratio of Beta in two Extracted Topics"}
coms_lda_td %>% 
  mutate(topic = paste0("topic", topic)) %>% 
  spread(topic, beta) %>% 
  dplyr::filter(topic1 > .001 | topic2 > .001) %>% 
  mutate(log_ratio = log2(topic2/topic1)) %>% 
  arrange(desc(abs(log_ratio))) %>% 
  top_n(20, abs(log_ratio)) %>% 
  ggplot(mapping = aes(x = term, y = log_ratio)) +
  geom_bar(stat = "identity") + coord_flip()
```

# Bibliografie
